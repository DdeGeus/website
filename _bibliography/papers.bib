---
---

@inproceedings{degeus2024tapps,
  abbr={CVPR},
  bibtex_show={true},
  title={{Task-aligned Part-aware Panoptic Segmentation through Joint Object-Part Representations}},
  author={{de Geus}, Daan and Dubbelman, Gijs},
  abstract={Part-aware panoptic segmentation (PPS) requires (a) that each foreground object and background region in an image is segmented and classified, and (b) that all parts within foreground objects are segmented, classified and linked to their parent object. Existing methods approach PPS by separately conducting object-level and part-level segmentation. However, their part-level predictions are not linked to individual parent objects. Therefore, their learning objective is not aligned with the PPS task objective, which harms the PPS performance. To solve this, and make more accurate PPS predictions, we propose Task-Aligned Part-aware Panoptic Segmentation (TAPPS). This method uses a set of shared queries to jointly predict (a) object-level segments, and (b) the part-level segments within those same objects. As a result, TAPPS learns to predict part-level segments that are linked to individual parent objects, aligning the learning objective with the task objective, and allowing TAPPS to leverage joint object-part representations. With experiments, we show that TAPPS considerably outperforms methods that predict objects and parts separately, and achieves new state-of-the-art PPS results.},
  booktitle={IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2024},
  pdf={https://tue-mps.github.io/tapps/resources/tapps_paper.pdf},
  project_page={https://tue-mps.github.io/tapps/},
  selected={true}
}


@inproceedings{norouzi2024algm,
  abbr={CVPR},
  bibtex_show={true},
  title={{ALGM: Adaptive Local-then-Global Token Merging for Efficient Semantic Segmentation with Plain Vision Transformers}},
  author={Norouzi, Narges and Orlova, Svetlana and {de Geus}, Daan and Dubbelman, Gijs},
  abstract={This work presents Adaptive Local-then-Global Merging (ALGM), a token reduction method for semantic segmentation networks that use plain Vision Transformers. ALGM merges tokens in two stages: (1) In the first network layer, it merges similar tokens within a small local window and (2) halfway through the network, it merges similar tokens across the entire image. This is motivated by an analysis in which we found that, in those situations, tokens with a high cosine similarity can likely be merged without a drop in segmentation quality. With extensive experiments across multiple datasets and network configurations, we show that ALGM not only significantly improves the throughput by up to 100\%, but can also enhance the mean IoU by up to +1.1, thereby achieving a better trade-off between segmentation quality and efficiency than existing methods. Moreover, our approach is adaptive during inference, meaning that the same model can be used for optimal efficiency or accuracy, depending on the application.}
  booktitle={IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2024},
  pdf={https://github.com/tue-mps/ALGM/blob/gh_pages/paper/ALGM__Adaptive_Local_Global_Token_Merging_for_Efficient_Semantic_Segmentation_in_Vision_Transformers_CVPR2024.pdf},
  project_page={https://tue-mps.github.io/ALGM/},
  selected={true}
}
