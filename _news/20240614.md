---
layout: post
title: I'll be at CVPR 2024 in Seattle! We'll be presenting 2 main papers and 2 workshop papers, come say hi!
date: 2024-06-14 12:00:00+0100
inline: false
related_posts: false
---

At CVPR 2024, the [Mobile Perception Systems Lab](https://www.tue-mps.org/) will present four new works! 

---

#### Task-aligned Part-aware Panoptic Segmentation through Joint Object-Part Representations (CVPR 2024 - Poster: Wednesday June 19 at 10:30 AM)
Part-aware panoptic segmentation (PPS) requires (a) that each foreground object and background region in an image is segmented and classified and (b) that all parts within foreground objects are segmented classified and linked to their parent object. Existing methods approach PPS by separately conducting object-level and part-level segmentation. However their part-level predictions are not linked to individual parent objects. Therefore their learning objective is not aligned with the PPS task objective which harms the PPS performance. To solve this and make more accurate PPS predictions we propose Task-Aligned Part-aware Panoptic Segmentation (TAPPS). This method uses a set of shared queries to jointly predict (a) object-level segments and (b) the part-level segments within those same objects. As a result TAPPS learns to predict part-level segments that are linked to individual parent objects aligning the learning objective with the task objective and allowing TAPPS to leverage joint object-part representations. With experiments we show that TAPPS considerably outperforms methods that predict objects and parts separately and achieves new state-of-the-art PPS results.

[[Paper](https://openaccess.thecvf.com/content/CVPR2024/papers/de_Geus_Task-aligned_Part-aware_Panoptic_Segmentation_through_Joint_Object-Part_Representations_CVPR_2024_paper.pdf)][[Project page](https://tue-mps.github.io/tapps/)]

