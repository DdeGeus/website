---
layout: post
title: I'll be at CVPR 2024 in Seattle! We'll be presenting 2 main papers and 2 workshop papers, come say hi!
date: 2024-06-14 12:00:00+0100
inline: false
related_posts: false
---

At CVPR 2024, the [Mobile Perception Systems Lab](https://www.tue-mps.org/) will present four new works! 

---

#### Task-aligned Part-aware Panoptic Segmentation through Joint Object-Part Representations (CVPR 2024 - Poster: Wednesday June 19 at 10:30 AM)
Part-aware panoptic segmentation (PPS) requires (a) that each foreground object and background region in an image is segmented and classified and (b) that all parts within foreground objects are segmented classified and linked to their parent object. Existing methods approach PPS by separately conducting object-level and part-level segmentation. However their part-level predictions are not linked to individual parent objects. Therefore their learning objective is not aligned with the PPS task objective which harms the PPS performance. To solve this and make more accurate PPS predictions we propose Task-Aligned Part-aware Panoptic Segmentation (TAPPS). This method uses a set of shared queries to jointly predict (a) object-level segments and (b) the part-level segments within those same objects. As a result TAPPS learns to predict part-level segments that are linked to individual parent objects aligning the learning objective with the task objective and allowing TAPPS to leverage joint object-part representations. With experiments we show that TAPPS considerably outperforms methods that predict objects and parts separately and achieves new state-of-the-art PPS results.

[[Paper](https://openaccess.thecvf.com/content/CVPR2024/papers/de_Geus_Task-aligned_Part-aware_Panoptic_Segmentation_through_Joint_Object-Part_Representations_CVPR_2024_paper.pdf)][[Project page](https://tue-mps.github.io/tapps/)]

---

#### ALGM: Adaptive Local-then-Global Token Merging for Efficient Semantic Segmentation with Plain Vision Transformers (CVPR 2024 - Poster: Thursday June 20 at 5:15 PM)
This work presents Adaptive Local-then-Global Merging (ALGM) a token reduction method for semantic segmentation networks that use plain Vision Transformers. ALGM merges tokens in two stages: (1) In the first network layer it merges similar tokens within a small local window and (2) halfway through the network it merges similar tokens across the entire image. This is motivated by an analysis in which we found that in those situations tokens with a high cosine similarity can likely be merged without a drop in segmentation quality. With extensive experiments across multiple datasets and network configurations we show that ALGM not only significantly improves the throughput by up to 100% but can also enhance the mean IoU by up to +1.1 thereby achieving a better trade-off between segmentation quality and efficiency than existing methods. Moreover our approach is adaptive during inference meaning that the same model can be used for optimal efficiency or accuracy depending on the application.

[[Paper](https://openaccess.thecvf.com/content/CVPR2024/papers/Norouzi_ALGM_Adaptive_Local-then-Global_Token_Merging_for_Efficient_Semantic_Segmentation_with_CVPR_2024_paper.pdf)][[Project page](https://tue-mps.github.io/ALGM/)]

---

#### Exploring the Benefits of Vision Foundation Models for Unsupervised Domain Adaptation (CVPR Workshops 2024 - Workshop on Foundation Models - Oral: Monday June 17 at 2:25 PM)
Achieving robust generalization across diverse data domains remains a significant challenge in computer vision. This challenge is important in safety-critical applications where deep-neural-network-based systems must perform reliably under various environmental conditions not seen during training. Our study investigates whether the generalization capabilities of Vision Foundation Models (VFMs) and Unsupervised Domain Adaptation (UDA) methods for the semantic segmentation task are complementary. Results show that combining VFMs with UDA has two main benefits: (a) it allows for better UDA performance while maintaining the out-of-distribution performance of VFMs and (b) it makes certain time-consuming UDA components redundant thus enabling significant inference speedups. Specifically with equivalent model sizes the resulting VFM-UDA method achieves an 8.4x speed increase over the prior non-VFM state of the art while also improving performance by +1.2 mIoU in the UDA setting and by +6.1 mIoU in terms of out-of-distribution generalization. Moreover when we use a VFM with 3.6x more parameters the VFM-UDA approach maintains a 3.3x speed up while improving the UDA performance by +3.1 mIoU and the out-of-distribution performance by +10.3 mIoU. These results underscore the significant benefits of combining VFMs with UDA setting new standards and baselines for Unsupervised Domain Adaptation in semantic segmentation.

[[Paper](https://openaccess.thecvf.com/content/CVPR2024W/2WFM/papers/Englert_Exploring_the_Benefits_of_Vision_Foundation_Models_for_Unsupervised_Domain_CVPRW_2024_paper.pdf)][[Code](https://tue-mps.github.io/ALGM/)]

---

#### How to Benchmark Vision Foundation Models for Semantic Segmentation? (CVPR Workshops 2024 - Workshop on Foundation Models - Poster: Monday June 17)
Recent vision foundation models (VFMs) have demonstrated proficiency in various tasks but require supervised fine-tuning to perform the task of semantic segmentation effectively. Benchmarking their performance is essential for selecting current models and guiding future model developments for this task. The lack of a standardized benchmark complicates comparisons. Therefore the primary objective of this paper is to study how VFMs should be benchmarked for semantic segmentation. To do so various VFMs are fine-tuned under various settings and the impact of individual settings on the performance ranking and training time is assessed. Based on the results the recommendation is to fine-tune the ViT-B variants of VFMs with a 16x16 patch size and a linear decoder as these settings are representative of using a larger model more advanced decoder and smaller patch size while reducing training time by more than 13 times. Using multiple datasets for training and evaluation is also recommended as the performance ranking across datasets and domain shifts varies. Linear probing a common practice for some VFMs is not recommended as it is not representative of end-to-end fine-tuning. The benchmarking setup recommended in this paper enables a performance analysis of VFMs for semantic segmentation. The findings of such an analysis reveal that pretraining with promptable segmentation is not beneficial whereas masked image modeling (MIM) with abstract representations is crucial even more important than the type of supervision used. The code for efficiently fine-tuning VFMs for semantic segmentation can be accessed through the project page at: [https://tue-mps.github.io/benchmark-vfm-ss/](https://tue-mps.github.io/benchmark-vfm-ss/).

[[Paper](https://openaccess.thecvf.com/content/CVPR2024W/2WFM/papers/Kerssies_How_to_Benchmark_Vision_Foundation_Models_for_Semantic_Segmentation_CVPRW_2024_paper.pdf)][[Code](https://github.com/tue-mps/vfm-uda)]
